---
layout:     post
title:      为流（streaming）世界重新设计的存储
subtitle:   
date:       2018-11-13
author:     Toby
header-img: img/cropped-lights2-1024x614.png
catalog: true
tags:
    - storage
    - stream
    - 流存储
---
# 为流（streaming）世界重新设计的存储

原文作者：Salvatore DeSimone
译者： Toby Huang

在将大量原始数据转化为有用信息和操作所需时间缩小到零的愿望的推动下，streaming看似简单 : 只需在数据到达时以快速、持续和无限的方式对其进行处理和操作。

对于从工业物联网到联网汽车到实时欺诈检测等，我们越来越多地寻求建立新的应用和客户体验，以快速响应客户的兴趣和行动，学习和适应不断变化的行为模式。 但实际情况是，我们大多数人还没有在生产级数据量上来完成采集率和故障恢复能力的工具。 因此，我们尽可能地利用定制系统在复杂性之上堆积复杂性。

复杂性是基本系统设计不匹配的症状: 我们使用的组件不是为它设计的, 我们所掌握的机制也不支持规模从小到大的扩展。

流化在规模上是困难的, 因为它具有三种破坏性系统功能:

- 能够将数据视为连续和无限的, 而不是有限和静态的

- 能够通过动态扩展数据采集、存储和处理能力, 与到达的数据量保持协调, 持续快速地交付结果

- 即使在延迟到达或出现乱序数据的情况下, 也能连续交付准确的结果处理数据

这里是它变得有趣, 甚至更具有破坏性的地方, 在一个很好的方面: 事件驱动、连续和有状态的数据处理的流式模式, 以及它对时间的一致理解, 在许多情况下比传统（ETL > Store > Query）的更高效和强大; *即使是没有实时要求的应用程序!*

![img](http://blog.pravega.io/wp-content/uploads/2017/04/Blog-Fig-1-1.png)

					图 1: The simple life of streaming

流式传输迫使系统设计人员重新思考基本的计算和存储原则。作为充满激情的存储人员, 我们正在尽自己的一份力量, 设计一个新的存储原语, 称为*stream*, 专为流式架构而构建, 并在一个名为*pravega*的新开源项目中实现.

通过将 Pravega 流存储与*apache flink*这样的有状态流处理器相结合, 我们实现了一个系统, 在这个系统中, 上面图片中的所有元素--写、处理、读和存储--都是独立的、弹性的,并与到达的数据量进行协调, 动态扩展, 使我们所有人都能构建以前无法构建的流式应用, 并将其从原型无缝扩展到生产。

## 流存储的要求

让我们看看流媒体系统的三个破坏性特征中的每一个, 看看 Pravega 流如何以今天存储无法实现的方式实现它们。

#### 将数据视为连续和无限

附加到文件的末尾并跟踪其内容模拟连续和无限的数据流, 但文件并没有针对此模式进行优化。它们也不是无限的。任何曾经旋转过日志文件的人都知道这一点。对于连续数据, 套接字或管道是更好的抽象, 但它们并不持久。消息传递是连续数据的合理抽象--尤其是kafka的append-only日志--但它们并不是作为无限、持久的系统而设计的。并且它们利用信包和标题来构造数据结构，使它们不像字节序列那样通用。

将这些想法拼接在一起, 我们提出了Pravega将从数据的角度支持的连续和无限的特点:

- Pravega流是一个命名的、持久的、仅追加的、无限的字节序列

- 使用低延迟追加并从序列的尾部读取

- 具有来自序列较旧部分的高吞吐赶超读取

  ![img](http://blog.pravega.io/wp-content/uploads/2017/04/Blog-Fig-2-1.png)			

  ​					图 2: Using streams in a pipeline

#### 基于数据到达量的系统扩展

那么, 我们如何根据数据量灵活地独立地扩展数据获取、存储和数据处理呢？

我们通过将数据拆分为多个分区并独立处理每个分区来获得并行性。例如, hadoop 使用 hdfs 和map-reduce实现了批处理。对于流式工作负载, 我们今天将使用队列或kafka分区来执行此操作。这两个选项都有相同的问题: 分区会影响读和写。连续处理的读写扩展要求通常不同, 链接它们会增加复杂性。此外, 虽然您可以添加队列或分区来扩展, 但这需要手动、协调地更新读、写和存储。这是复杂的, 而不是动态缩放。

Pravega流， 为动态和独立扩展而设计，支持:

- 许多写入者同时追加一个不相交的数据子集
  - 不相交的子集是由使用相同键写入的数据定义的
  - 为写入者分配键值留给应用–当键值空间或写入者更改时, 存储不得约束或要求更改
- 许多读者同时处理不相交的数据子集
  - 读取的数据分区必须独立于写入分区
  - 读取分区必须由存储策略控制, 例如, 将流拆分为足够的段, 以确保没有一个流看到超过 N 字节/秒
  - 存储系统必须根据每个传入数据量自动和连续地更新流中的段数

这些都是苛刻的要求。让我们来看看两个典型的分区方案。

![img](http://blog.pravega.io/wp-content/uploads/2017/04/Blog-Fig-3-1.png)

					图 3: Ingest rate << Processing rate



在图3中, 处理时间比获取长。有一个写入器, 但数据是为读取而分段的: 读取器 #1 获取键值 ka .. kc,的数据, 另一个得到 kd .. kf。在图4中, 处理比接收更快, 因此拓扑倒置: 多个编写器对写入的键空间进行分区, 但一个读取器对其进行处理。

![img](http://blog.pravega.io/wp-content/uploads/2017/04/Blog-Fig-4-1.png)

					图 4: Ingest rate >> Processing rate

在现实生活中, 我们最终处于两者之间--随着数据源和应用的发展, 我们可能会随着时间的推移而变化。虽然流内部将由多个段组成, 但 (a) 编写者不知道段拓扑, 因为他们只知道键, (b) 读者动态地学习段拓扑--只需将它们指向流即可。

为了使整个系统 (存储 + 处理) 适应不断变化的数据量, Pravega 不断监视流的传入数据速率, 并确保存在适当数量的段, 以满足 SLO 合规性。图5说明了流的段随着时间的推移而动态变化。

![img](http://blog.pravega.io/wp-content/uploads/2017/04/Blog-Fig-5-1.png)

					图 5:  Dynamically scaling stream segments over time

在*t0*, 传入的数据速率低于缩放SLO，所有数据都存储在*段0*中。在*t1*, 超出SLO，*段*0 被密封, 并创建*段1*和*段2*。新数据*K0*和*K1*将转到*段2*。新数据*K2*和*K3*转到*段*1。这是一个*段*分割, 以响应数据量增加。拆分也发生在*t2*和*t3*.

在*t4*, 速度变慢。*段3*和*段6*被密封,*段7*被创建, 并将保存新的数据*k1 .. k2*.这是一个*段合并*, 以响应数据量的减少。

Pravega段缩放协议允许读者跟踪段缩放并采取适当的行动, 例如添加或删除读取器, 使整个系统能够以协调的方式动态扩展。

![img](http://blog.pravega.io/wp-content/uploads/2017/04/Blog-Fig-6-1.png)	

					图 6: A streaming system with dynamic, coordinated scaling of components

#### 连续处理数据生成准确的结果

连续计算准确的结果*意味着只需一次*处理, 并且当有关该事件的数据被处理计算时，将*事件*时间 (现实生活中发生的时间) 与*处理*时间区分开来。为此, 我们添加了链接应用的要求, 同时只保留一次, 以便将计算拆分为多个独立的应用程序。这就是流式系统微服务。

与实时和批处理使用独立基础架构的*lambda 体系结构*相比, 产生准确结果的流系统可显著节约成本。这不仅更简单、更便宜--它是一个基础架构, 而不是两个基础架构--它简化了开发, 因为您对 lambda 中*的每个基础结构*进行了一次编码, 而不是一次。在 Tyler Akidau 的 O’Reilly 博客中, 有一个非常棒的概念： [The world beyond batch: Streaming 101](https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101)..

一次的存储要求是明确的: 流必须是持久的、有序的、一致的和事务性的。这些都是关键属性, 因为它们是存储系统设计中最困难的方面。如果没有重大的重新设计, 你以后就不能改变它们。

持久性意味着, 一旦得到承认, 即使面对组件故障, 写入也永远不会丢失。持久性是必不可少的, 只有一次, 因为如果数据丢失, 就不能 (重新) 处理。*大多数持久*的数据并不能减少它: 要么你可以依靠存储来实现持久性, 要么你不能。不持久的系统不是一个记录系统, 这意味着数据的*永久副本*必须存储在其他位置--通常是在存档系统 (如对象存储或 nas) 中。存档是指ETL的应用代码和 ETL 流程的管理。这种复杂性被消除了, 因为Pravega流存储是一个持久的永久存储, 您可以在这里可靠地永远保留流数据。

有序意味着读取器将以相同的写入顺序处理数据。对于像具有键分区写入的流这样的系统, 排序仅对具有相同键值的数据有意义。在拥有数百万台设备生成传感器指标的物联网系统中,*传感器-id.*指标可能是一个键值。流保证对键值数据的读取将以其写入的顺序进行。对于许多计算 (如使用增量更新计算的聚合指标), 排序是必不可少的。

一致性意味着*所有读者都将看到给定键值的相同有序数据*视图 (即使在组件故障面前也是如此), 无论*数据是从流的尾部读取还*是*通过赶超读取读取*。与持久性一样,*大多数一致*并不能减少它: 要么存储是一致的, 要么不是一致的。从一次性需求的角度来看, 存储一致性与区分计算层中的事件时间和处理时间同样重要。

在链式应用中, 事务写入只需一次。像 flink 这样的有状态流处理器*在使用*巧妙的分布式检查点的单个应用中具有正好一次（*exactly once*）的内部机制。将范围扩展到多个应用中, 需要中间存储 (在我们的示例中为流) 通过事务写入参与这些检查点方案。

## Pravega流介绍

Pravega是实现流的开源分布式存储服务。流是可靠的流式传输系统的基础: 高性能、持久弹性，无限和只追加，具有严格的顺序和一致性的的字节流.

流是轻量级的。就像文件或对象一样, 我们可以根据需要快速、轻松地创建任意数量的文件或对象--在单个集群中按数百万的顺序创建。

通过重构和外部化以前的内部日志和专有日志, 流大大简化了新一代分布式中间件的开发和操作, 将其*重新构想为流式基础结构*:

![img](http://blog.pravega.io/wp-content/uploads/2017/04/Blog-Fig-7-1.png)	

					图 7: Refactoring the Stack for a Streaming World

Pravega 项目目前包括Pravega字节流原语以及分层获取缓冲区和pub / sub机制，在概念上与Kafka类似，但具有性能、弹性、无限性、一致性和持久性的流特性。我们将在下一节中讨论Pravega的获取缓冲区与 flink 的集成。

另外两个项目都将常见中间件服务重新想象为流式传输基础结构, 它们处于早期概念阶段:

- 基于流的全文搜索: 动态、分布式、实时 lucene 索引器, 具有流数据的连续查询功能
- 流支持的持久数据结构: 一个微服务纯粹主义者的框架, 他们希望自己的微服务拥有*自己的数据*

敬请关注此博客空间, 了解有关这些项目的更多信息!

## Pravega架构

Pravega 的体系结构有三个主要组件。*Pravega 流服务*是一种分布式软件服务, 实现了流抽象语义, 包括流控制和段存储 APIs、数据内存缓存 (使用*roks db)*以及数据放置和分层利用两个基础存储系统的逻辑: 使用*Apache Bookkeeper*的低延迟存储和使用*HDFS*支持高吞吐量、大规模存储。[此组件设计为可插入, 以支持具有适当强一致性语义的备用支持存储系统。

![img](http://blog.pravega.io/wp-content/uploads/2017/04/Blog-Fig-8-1.png)

					图 8: Pravega Streaming Storage Architecture

在Pravega 的系统设计中, 有许多创新, 使其能够满足流的挑战性要求。 I/O 路径设计完全隔离了读取和写入路径, 从而实现了对尾部的极低延迟持久写入、从尾部读取的低延迟读取以及从流的较旧部分读取的高吞吐量。 Pravega 体系结构的细节超出了本文的范围。更多信息可在体系结构 wiki 中找到。

#### Streaming Storage + Apache Flink = YEAH!

让我们来看看Pravega 流如何与 flink 集成, 以实现动态和弹性系统, 提供快速、准确的计算结果, 同时在恒定的时间内处理海量数据, 即使数据速率也会千差万别。

系统的概念结构如图9所示。它包含一个典型的输入流, 其中包含由编写者组成的团队编写的原始数据、一个处理它的多工作 flink 应用程序以及一个处理第一个应用程序输出的链接 flink 应用程序。

![img](http://blog.pravega.io/wp-content/uploads/2017/04/Blog-Fig-9a.png)

					图9： Pravega + Flink

这是不同的地方: 每个元素--编写器、输入流、读取器应用程序、输出流--都是*独立的、弹性的, 并且可以动态扩展*, 以响应数据量到达率随时间的变化。

有两个集成点支持这一点: Pravega 的细分缩放可驱动 flink 的工作人员缩放, 以及通过流链接应用, 在整个系统中只保留一次。仅使用一名工作人员部署 flink 应用, 并根据流 slo 动态扩展该应用。好！普拉维加和 flink 开发人员已经在将流自动缩放集成到 flink 中。

除了这种好处之外, 无限流大大简化了许多操作用例。让我们考虑推出一个新版本的 flink 应用 (任何应用程序, 真的), 您首先希望在其中使用历史数据对其进行测试。

图10显示了当今实时 flink 应用程序的典型部署. 信息被输入邮件系统, 由 flink 应用程序处理, 然后转发到 noc 或类似的框架以显示和/或操作。同时, ETL 工作人员会不断地提取消息并将其写入持久存储区, 以便进行历史访问。

![img](http://blog.pravega.io/wp-content/uploads/2017/04/Blog-Fig-10-1.png)

					图 10: Complexity of testing a new app version without streams

现在我们已经构建了一个新版本的App, App '。针对历史数据集尝试新逻辑以验证正确性并确保在将无中断的情况下部署到生产环境之前不发生倒退的操作过程是什么？

首先, 我们需要部署 App ', 以便从存档而不是邮件系统获取其数据。因此, 您的测试*与生产不完全一样*: 存档和消息之间的细微行为差异可能会使测试不可靠。测试完成后, 我们将重新部署 App ' 以使用邮件系统, 并重新优化其缓存或从历史数据派生的其他状态。如果一切正常, 我们终于可以更换以前的版本了。其结果是一个复杂的工作流序列。复杂性意味着麻烦。

这种情况与Pravega流发生了怎样的变化？App' 的部署*与生产完全一样,*因为历史数据是通过*相同的流*访问的--只需将其倒带!使用历史记录时, "App" 和 "App'" 将以相同的状态处理相同的数据。当我们确信App' 是好的, 关闭应用程序, 并重定向 NOC。Yeah！

![img](http://blog.pravega.io/wp-content/uploads/2017/04/Blog-Fig-11-1.png)图 11: Testing a new app version with streams

## 篇尾寄语

对于那些走到最后的人...... 谢谢你的耐心!我们是充满激情的存储人员, 我们喜欢从正确的存储结构如何帮助解决问题的角度来看待问题。我们喜欢流媒体的想法。我们对 flink 这样的计算技术感到兴奋, 这些技术是 "天生的流媒体"。我们认为世界需要一种由流媒体诞生的互补存储技术。Pravega是我们的贡献。我们相信, 它将进一步发展流媒体的最先进, 使其好处容易为任何人。

请记住, 当您想到流式传输应用程序时, 请将数据视为连续和无限的, 而不是静态和有限的。想想企业存储质量的重要性, 比如持久性、一致性、*弹性, 现在是无穷大*(这是一个词吗？)。

请经常查看此博客, 以跟上与 pravega 将数据中间件重新设想为流式传输基础结构的使命相关的新概念和新想法。如果你也对此充满激情, 我们鼓励你加入我们的社区!

[Ref]: http://blog.pravega.io/2017/04/	"Storage Reimagined for a Streaming World"


